{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Referenznotebook zum Thema Support Vector Machines. Für die vollständigen Erklärungen bitte auch das Video nutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einlesen\n",
    "\n",
    "Wir verwenden hier wieder den Iris Datensatz, da wir diesen schon kennen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.9</td><td>3</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa\\\\\n",
       "\t2 & 4.9 & 3 & 1.4 & 0.2 & setosa\\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa\\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa\\\\\n",
       "\t5 & 5 & 3.6 & 1.4 & 0.2 & setosa\\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1          5.1         3.5          1.4         0.2  setosa\n",
       "2          4.9         3.0          1.4         0.2  setosa\n",
       "3          4.7         3.2          1.3         0.2  setosa\n",
       "4          4.6         3.1          1.5         0.2  setosa\n",
       "5          5.0         3.6          1.4         0.2  setosa\n",
       "6          5.4         3.9          1.7         0.4  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Modell erstellen\n",
    "\n",
    "Wir verwenden hier die `e1071` Library. [Mehr Info hier.](https://cran.r-project.org/web/packages/e1071/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages('e1071',repos = 'http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(e1071)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können hier die `svm()` Funktion verwenden um das Modell zu erstellen und um das SVM Modell zu trainieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for svm {e1071}\"><tr><td>svm {e1071}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Support Vector Machines</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p><code>svm</code> is used to train a support vector machine. It can be used to carry\n",
       "out general regression and classification (of nu and epsilon-type), as\n",
       "well as density-estimation. A formula interface is provided.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "## S3 method for class 'formula'\n",
       "svm(formula, data = NULL, ..., subset, na.action =\n",
       "na.omit, scale = TRUE)\n",
       "## Default S3 method:\n",
       "svm(x, y = NULL, scale = TRUE, type = NULL, kernel =\n",
       "\"radial\", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),\n",
       "coef0 = 0, cost = 1, nu = 0.5,\n",
       "class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,\n",
       "shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,\n",
       "..., subset, na.action = na.omit)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>formula</code></td>\n",
       "<td>\n",
       "<p>a symbolic description of the model to be fit.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>data</code></td>\n",
       "<td>\n",
       "<p>an optional data frame containing the variables in the model.\n",
       "By default the variables are taken from the environment which\n",
       "&lsquo;svm&rsquo; is called from.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>x</code></td>\n",
       "<td>\n",
       "<p>a data matrix, a vector, or a sparse matrix (object of class\n",
       "<code>Matrix</code> provided by the <span class=\"pkg\">Matrix</span> package,\n",
       "or of class <code>matrix.csr</code>\n",
       "provided by the <span class=\"pkg\">SparseM</span> package, or of class\n",
       "<code>simple_triplet_matrix</code> provided by the <span class=\"pkg\">slam</span>\n",
       "package).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "<p>a response vector with one label for each row/component of\n",
       "<code>x</code>. Can be either a factor (for classification tasks)\n",
       "or a numeric vector (for regression).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>scale</code></td>\n",
       "<td>\n",
       "<p>A logical vector indicating the variables to be\n",
       "scaled. If <code>scale</code> is of length 1, the value is recycled as\n",
       "many times as needed.\n",
       "Per default, data are scaled internally (both <code>x</code> and <code>y</code>\n",
       "variables) to zero mean and unit variance. The center and scale\n",
       "values are returned and used for later predictions.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type</code></td>\n",
       "<td>\n",
       "<p><code>svm</code> can be used as a classification\n",
       "machine, as a regression machine, or for novelty detection.\n",
       "Depending of whether <code>y</code> is\n",
       "a factor or not, the default setting for <code>type</code> is <code>C-classification</code> or <code>eps-regression</code>, respectively, but may be overwritten by setting an explicit value.<br />\n",
       "Valid options are:\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li> <p><code>C-classification</code>\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>nu-classification</code>\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>one-classification</code> (for novelty detection)\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>eps-regression</code>\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>nu-regression</code>\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>kernel</code></td>\n",
       "<td>\n",
       "<p>the kernel used in training and predicting. You\n",
       "might consider changing some of the following parameters, depending\n",
       "on the kernel type.<br />\n",
       "</p>\n",
       "\n",
       "<dl>\n",
       "<dt>linear:</dt><dd><p><i>u'*v</i></p>\n",
       "</dd>\n",
       "<dt>polynomial:</dt><dd><p><i>(gamma*u'*v + coef0)^degree</i></p>\n",
       "</dd>\n",
       "<dt>radial basis:</dt><dd><p><i>exp(-gamma*|u-v|^2)</i></p>\n",
       "</dd>\n",
       "<dt>sigmoid:</dt><dd><p><i>tanh(gamma*u'*v + coef0)</i></p>\n",
       "</dd>\n",
       "</dl>\n",
       "\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>degree</code></td>\n",
       "<td>\n",
       "<p>parameter needed for kernel of type <code>polynomial</code> (default: 3)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>gamma</code></td>\n",
       "<td>\n",
       "<p>parameter needed for all kernels except <code>linear</code>\n",
       "(default: 1/(data dimension))</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>coef0</code></td>\n",
       "<td>\n",
       "<p>parameter needed for kernels of type <code>polynomial</code>\n",
       "and <code>sigmoid</code> (default: 0)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cost</code></td>\n",
       "<td>\n",
       "<p>cost of constraints violation (default: 1)&mdash;it is the\n",
       "&lsquo;C&rsquo;-constant of the regularization term in the Lagrange formulation.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nu</code></td>\n",
       "<td>\n",
       "<p>parameter needed for <code>nu-classification</code>,\n",
       "<code>nu-regression</code>, and <code>one-classification</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>class.weights</code></td>\n",
       "<td>\n",
       "<p>a named vector of weights for the different\n",
       "classes, used for asymmetric class sizes. Not all factor levels have\n",
       "to be supplied (default weight: 1). All components have to be named.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cachesize</code></td>\n",
       "<td>\n",
       "<p>cache memory in MB (default 40)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>tolerance</code></td>\n",
       "<td>\n",
       "<p>tolerance of termination criterion (default: 0.001)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>epsilon</code></td>\n",
       "<td>\n",
       "<p>epsilon in the insensitive-loss function (default: 0.1)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>shrinking</code></td>\n",
       "<td>\n",
       "<p>option whether to use the shrinking-heuristics\n",
       "(default: <code>TRUE</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cross</code></td>\n",
       "<td>\n",
       "<p>if a integer value k&gt;0 is specified, a k-fold cross\n",
       "validation on the training data is performed to assess the quality\n",
       "of the model: the accuracy rate for classification and the Mean\n",
       "Squared Error for regression</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fitted</code></td>\n",
       "<td>\n",
       "<p>logical indicating whether the fitted values should be computed\n",
       "and included in the model or not (default: <code>TRUE</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>probability</code></td>\n",
       "<td>\n",
       "<p>logical indicating whether the model should\n",
       "allow for probability predictions.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>additional parameters for the low level fitting function\n",
       "<code>svm.default</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>subset</code></td>\n",
       "<td>\n",
       "<p>An index vector specifying the cases to be used in the\n",
       "training sample.  (NOTE: If given, this argument must be\n",
       "named.)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>na.action</code></td>\n",
       "<td>\n",
       "<p>A function to specify the action to be taken if <code>NA</code>s are\n",
       "found. The default action is <code>na.omit</code>, which leads to rejection of cases\n",
       "with missing values on any required variable. An alternative\n",
       "is <code>na.fail</code>, which causes an error if <code>NA</code> cases\n",
       "are found. (NOTE: If given, this argument must be named.)</p>\n",
       "</td></tr>\t\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>For multiclass-classification with k levels, k&gt;2, <code>libsvm</code> uses the\n",
       "&lsquo;one-against-one&rsquo;-approach, in which k(k-1)/2 binary classifiers are\n",
       "trained; the appropriate class is found by a voting scheme.\n",
       "</p>\n",
       "<p><code>libsvm</code> internally uses a sparse data representation, which is \n",
       "also high-level supported by the package <span class=\"pkg\">SparseM</span>.\n",
       "</p>\n",
       "<p>If the predictor variables include factors, the formula interface must be used to get a\n",
       "correct model matrix.\n",
       "</p>\n",
       "<p><code>plot.svm</code> allows a simple graphical\n",
       "visualization of classification models.\n",
       "</p>\n",
       "<p>The probability model for classification fits a logistic distribution\n",
       "using maximum likelihood to the decision values of all binary\n",
       "classifiers, and computes the a-posteriori class probabilities for the\n",
       "multi-class problem using quadratic optimization. The probabilistic\n",
       "regression model assumes (zero-mean) laplace-distributed errors for the\n",
       "predictions, and estimates the scale parameter using maximum likelihood.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>An object of class <code>\"svm\"</code> containing the fitted model, including:\n",
       "</p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>SV</code></td>\n",
       "<td>\n",
       "<p>The resulting support vectors (possibly scaled).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>index</code></td>\n",
       "<td>\n",
       "<p>The index of the resulting support vectors in the data\n",
       "matrix. Note that this index refers to the preprocessed data (after\n",
       "the possible effect of <code>na.omit</code> and <code>subset</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>coefs</code></td>\n",
       "<td>\n",
       "<p>The corresponding coefficients times the training labels.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>rho</code></td>\n",
       "<td>\n",
       "<p>The negative intercept.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>sigma</code></td>\n",
       "<td>\n",
       "<p>In case of a probabilistic regression model, the scale\n",
       "parameter of the hypothesized (zero-mean) laplace distribution estimated by\n",
       "maximum likelihood.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>probA, probB</code></td>\n",
       "<td>\n",
       "<p>numeric vectors of length k(k-1)/2, k number of\n",
       "classes, containing the parameters of the logistic distributions fitted to\n",
       "the decision values of the binary classifiers (1 / (1 + exp(a x + b))).</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Note</h3>\n",
       "\n",
       "<p>Data are scaled internally, usually yielding better results.\n",
       "</p>\n",
       "<p>Parameters of SVM-models usually <em>must</em> be tuned to yield sensible results!\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin)<br />\n",
       "<a href=\"mailto:David.Meyer@R-project.org\">David.Meyer@R-project.org</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "\n",
       "<ul>\n",
       "<li>\n",
       "<p>Chang, Chih-Chung and Lin, Chih-Jen:<br />\n",
       "<em>LIBSVM: a library for Support Vector Machines</em><br />\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm\">http://www.csie.ntu.edu.tw/~cjlin/libsvm</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>Exact formulations of models, algorithms, etc. can be found in the\n",
       "document:<br />\n",
       "Chang, Chih-Chung and Lin, Chih-Jen:<br />\n",
       "<em>LIBSVM: a library for Support Vector Machines</em><br />\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz\">http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>More implementation details and speed benchmarks can be found on:\n",
       "Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:<br />\n",
       "<em>Working Set Selection Using the Second Order Information for Training SVM</em><br />\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf\">http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf</a>\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>predict.svm</code>\n",
       "<code>plot.svm</code>\n",
       "<code>tune.svm</code>\n",
       "<code>matrix.csr</code> (in package <span class=\"pkg\">SparseM</span>)\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "data(iris)\n",
       "attach(iris)\n",
       "\n",
       "## classification mode\n",
       "# default with factor response:\n",
       "model &lt;- svm(Species ~ ., data = iris)\n",
       "\n",
       "# alternatively the traditional interface:\n",
       "x &lt;- subset(iris, select = -Species)\n",
       "y &lt;- Species\n",
       "model &lt;- svm(x, y) \n",
       "\n",
       "print(model)\n",
       "summary(model)\n",
       "\n",
       "# test with train data\n",
       "pred &lt;- predict(model, x)\n",
       "# (same as:)\n",
       "pred &lt;- fitted(model)\n",
       "\n",
       "# Check accuracy:\n",
       "table(pred, y)\n",
       "\n",
       "# compute decision values and probabilities:\n",
       "pred &lt;- predict(model, x, decision.values = TRUE)\n",
       "attr(pred, \"decision.values\")[1:4,]\n",
       "\n",
       "# visualize (classes by color, SV by crosses):\n",
       "plot(cmdscale(dist(iris[,-5])),\n",
       "     col = as.integer(iris[,5]),\n",
       "     pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n",
       "\n",
       "## try regression mode on two dimensions\n",
       "\n",
       "# create data\n",
       "x &lt;- seq(0.1, 5, by = 0.05)\n",
       "y &lt;- log(x) + rnorm(x, sd = 0.2)\n",
       "\n",
       "# estimate model and predict input values\n",
       "m   &lt;- svm(x, y)\n",
       "new &lt;- predict(m, x)\n",
       "\n",
       "# visualize\n",
       "plot(x, y)\n",
       "points(x, log(x), col = 2)\n",
       "points(x, new, col = 4)\n",
       "\n",
       "## density-estimation\n",
       "\n",
       "# create 2-dim. normal with rho=0:\n",
       "X &lt;- data.frame(a = rnorm(1000), b = rnorm(1000))\n",
       "attach(X)\n",
       "\n",
       "# traditional way:\n",
       "m &lt;- svm(X, gamma = 0.1)\n",
       "\n",
       "# formula interface:\n",
       "m &lt;- svm(~., data = X, gamma = 0.1)\n",
       "# or:\n",
       "m &lt;- svm(~ a + b, gamma = 0.1)\n",
       "\n",
       "# test:\n",
       "newdata &lt;- data.frame(a = c(0, 4), b = c(0, 4))\n",
       "predict (m, newdata)\n",
       "\n",
       "# visualize:\n",
       "plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))\n",
       "points(newdata, pch = \"+\", col = 2, cex = 5)\n",
       "\n",
       "# weights: (example not particularly sensible)\n",
       "i2 &lt;- iris\n",
       "levels(i2$Species)[3] &lt;- \"versicolor\"\n",
       "summary(i2$Species)\n",
       "wts &lt;- 100 / table(i2$Species)\n",
       "wts\n",
       "m &lt;- svm(Species ~ ., data = i2, class.weights = wts)\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>e1071</em> version 1.6-7 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{svm}{Support Vector Machines}{svm}\n",
       "\\aliasA{print.summary.svm}{svm}{print.summary.svm}\n",
       "\\aliasA{print.svm}{svm}{print.svm}\n",
       "\\aliasA{summary.svm}{svm}{summary.svm}\n",
       "\\methaliasA{svm.default}{svm}{svm.default}\n",
       "\\methaliasA{svm.formula}{svm}{svm.formula}\n",
       "\\keyword{neural}{svm}\n",
       "\\keyword{nonlinear}{svm}\n",
       "\\keyword{classif}{svm}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "\\code{svm} is used to train a support vector machine. It can be used to carry\n",
       "out general regression and classification (of nu and epsilon-type), as\n",
       "well as density-estimation. A formula interface is provided.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "## S3 method for class 'formula'\n",
       "svm(formula, data = NULL, ..., subset, na.action =\n",
       "na.omit, scale = TRUE)\n",
       "## Default S3 method:\n",
       "svm(x, y = NULL, scale = TRUE, type = NULL, kernel =\n",
       "\"radial\", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),\n",
       "coef0 = 0, cost = 1, nu = 0.5,\n",
       "class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,\n",
       "shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,\n",
       "..., subset, na.action = na.omit)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{formula}] a symbolic description of the model to be fit.\n",
       "\\item[\\code{data}] an optional data frame containing the variables in the model.\n",
       "By default the variables are taken from the environment which\n",
       "`svm' is called from.\n",
       "\\item[\\code{x}] a data matrix, a vector, or a sparse matrix (object of class\n",
       "\\code{\\LinkA{Matrix}{Matrix}} provided by the \\pkg{Matrix} package,\n",
       "or of class \\code{\\LinkA{matrix.csr}{matrix.csr}}\n",
       "provided by the \\pkg{SparseM} package, or of class\n",
       "\\code{\\LinkA{simple\\_triplet\\_matrix}{simple.Rul.triplet.Rul.matrix}} provided by the \\pkg{slam}\n",
       "package).\n",
       "\\item[\\code{y}] a response vector with one label for each row/component of\n",
       "\\code{x}. Can be either a factor (for classification tasks)\n",
       "or a numeric vector (for regression).\n",
       "\\item[\\code{scale}] A logical vector indicating the variables to be\n",
       "scaled. If \\code{scale} is of length 1, the value is recycled as\n",
       "many times as needed.\n",
       "Per default, data are scaled internally (both \\code{x} and \\code{y}\n",
       "variables) to zero mean and unit variance. The center and scale\n",
       "values are returned and used for later predictions.\n",
       "\\item[\\code{type}] \\code{svm} can be used as a classification\n",
       "machine, as a regression machine, or for novelty detection.\n",
       "Depending of whether \\code{y} is\n",
       "a factor or not, the default setting for \\code{type} is \\code{C-classification} or \\code{eps-regression}, respectively, but may be overwritten by setting an explicit value.\\\\{}\n",
       "Valid options are:\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item \\code{C-classification}\n",
       "\\item \\code{nu-classification}\n",
       "\\item \\code{one-classification} (for novelty detection)\n",
       "\\item \\code{eps-regression}\n",
       "\\item \\code{nu-regression}\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "\\item[\\code{kernel}] the kernel used in training and predicting. You\n",
       "might consider changing some of the following parameters, depending\n",
       "on the kernel type.\\\\{}\n",
       "\\begin{description}\n",
       "\n",
       "\\item[linear:] \\eqn{u'v}{}\n",
       "\\item[polynomial:] \\eqn{(\\gamma u'v + coef0)^{degree}}{}\n",
       "\\item[radial basis:] \\eqn{e^(-\\gamma |u-v|^2)}{}\n",
       "\\item[sigmoid:] \\eqn{tanh(\\gamma u'v + coef0)}{}\n",
       "\n",
       "\\end{description}\n",
       "\n",
       "\n",
       "\\item[\\code{degree}] parameter needed for kernel of type \\code{polynomial} (default: 3)\n",
       "\\item[\\code{gamma}] parameter needed for all kernels except \\code{linear}\n",
       "(default: 1/(data dimension))\n",
       "\\item[\\code{coef0}] parameter needed for kernels of type \\code{polynomial}\n",
       "and \\code{sigmoid} (default: 0)\n",
       "\\item[\\code{cost}] cost of constraints violation (default: 1)---it is the\n",
       "`C'-constant of the regularization term in the Lagrange formulation.\n",
       "\\item[\\code{nu}] parameter needed for \\code{nu-classification},\n",
       "\\code{nu-regression}, and \\code{one-classification}\n",
       "\\item[\\code{class.weights}] a named vector of weights for the different\n",
       "classes, used for asymmetric class sizes. Not all factor levels have\n",
       "to be supplied (default weight: 1). All components have to be named.\n",
       "\\item[\\code{cachesize}] cache memory in MB (default 40)\n",
       "\\item[\\code{tolerance}] tolerance of termination criterion (default: 0.001)\n",
       "\\item[\\code{epsilon}] epsilon in the insensitive-loss function (default: 0.1)\n",
       "\\item[\\code{shrinking}] option whether to use the shrinking-heuristics\n",
       "(default: \\code{TRUE})\n",
       "\\item[\\code{cross}] if a integer value k>0 is specified, a k-fold cross\n",
       "validation on the training data is performed to assess the quality\n",
       "of the model: the accuracy rate for classification and the Mean\n",
       "Squared Error for regression\n",
       "\\item[\\code{fitted}] logical indicating whether the fitted values should be computed\n",
       "and included in the model or not (default: \\code{TRUE})\n",
       "\\item[\\code{probability}] logical indicating whether the model should\n",
       "allow for probability predictions.\n",
       "\\item[\\code{...}] additional parameters for the low level fitting function\n",
       "\\code{svm.default}\n",
       "\\item[\\code{subset}] An index vector specifying the cases to be used in the\n",
       "training sample.  (NOTE: If given, this argument must be\n",
       "named.)\n",
       "\\item[\\code{na.action}] A function to specify the action to be taken if \\code{NA}s are\n",
       "found. The default action is \\code{na.omit}, which leads to rejection of cases\n",
       "with missing values on any required variable. An alternative\n",
       "is \\code{na.fail}, which causes an error if \\code{NA} cases\n",
       "are found. (NOTE: If given, this argument must be named.)\t\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "For multiclass-classification with k levels, k>2, \\code{libsvm} uses the\n",
       "`one-against-one'-approach, in which k(k-1)/2 binary classifiers are\n",
       "trained; the appropriate class is found by a voting scheme.\n",
       "\n",
       "\\code{libsvm} internally uses a sparse data representation, which is \n",
       "also high-level supported by the package \\pkg{SparseM}.\n",
       "\n",
       "If the predictor variables include factors, the formula interface must be used to get a\n",
       "correct model matrix.\n",
       "\n",
       "\\code{plot.svm} allows a simple graphical\n",
       "visualization of classification models.\n",
       "\n",
       "The probability model for classification fits a logistic distribution\n",
       "using maximum likelihood to the decision values of all binary\n",
       "classifiers, and computes the a-posteriori class probabilities for the\n",
       "multi-class problem using quadratic optimization. The probabilistic\n",
       "regression model assumes (zero-mean) laplace-distributed errors for the\n",
       "predictions, and estimates the scale parameter using maximum likelihood.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "An object of class \\code{\"svm\"} containing the fitted model, including:\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{SV}] The resulting support vectors (possibly scaled).\n",
       "\\item[\\code{index}] The index of the resulting support vectors in the data\n",
       "matrix. Note that this index refers to the preprocessed data (after\n",
       "the possible effect of \\code{na.omit} and \\code{subset})\n",
       "\\item[\\code{coefs}] The corresponding coefficients times the training labels.\n",
       "\\item[\\code{rho}] The negative intercept.\n",
       "\\item[\\code{sigma}] In case of a probabilistic regression model, the scale\n",
       "parameter of the hypothesized (zero-mean) laplace distribution estimated by\n",
       "maximum likelihood.\n",
       "\\item[\\code{probA, probB}] numeric vectors of length k(k-1)/2, k number of\n",
       "classes, containing the parameters of the logistic distributions fitted to\n",
       "the decision values of the binary classifiers (1 / (1 + exp(a x + b))).\n",
       "\\end{ldescription}\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Note}\\relax\n",
       "Data are scaled internally, usually yielding better results.\n",
       "\n",
       "Parameters of SVM-models usually \\emph{must} be tuned to yield sensible results!\n",
       "\\end{Note}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen Lin)\\\\{}\n",
       "\\email{David.Meyer@R-project.org}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item \n",
       "Chang, Chih-Chung and Lin, Chih-Jen:\\\\{}\n",
       "\\emph{LIBSVM: a library for Support Vector Machines}\\\\{}\n",
       "\\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}\n",
       "\n",
       "\\item \n",
       "Exact formulations of models, algorithms, etc. can be found in the\n",
       "document:\\\\{}\n",
       "Chang, Chih-Chung and Lin, Chih-Jen:\\\\{}\n",
       "\\emph{LIBSVM: a library for Support Vector Machines}\\\\{}\n",
       "\\url{http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz}\n",
       "\n",
       "\\item \n",
       "More implementation details and speed benchmarks can be found on:\n",
       "Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:\\\\{}\n",
       "\\emph{Working Set Selection Using the Second Order Information for Training SVM}\\\\{}\n",
       "\\url{http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf}\n",
       "\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{predict.svm}{predict.svm}}\n",
       "\\code{\\LinkA{plot.svm}{plot.svm}}\n",
       "\\code{\\LinkA{tune.svm}{tune.svm}}\n",
       "\\code{\\LinkA{matrix.csr}{matrix.csr}} (in package \\pkg{SparseM})\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "data(iris)\n",
       "attach(iris)\n",
       "\n",
       "## classification mode\n",
       "# default with factor response:\n",
       "model <- svm(Species ~ ., data = iris)\n",
       "\n",
       "# alternatively the traditional interface:\n",
       "x <- subset(iris, select = -Species)\n",
       "y <- Species\n",
       "model <- svm(x, y) \n",
       "\n",
       "print(model)\n",
       "summary(model)\n",
       "\n",
       "# test with train data\n",
       "pred <- predict(model, x)\n",
       "# (same as:)\n",
       "pred <- fitted(model)\n",
       "\n",
       "# Check accuracy:\n",
       "table(pred, y)\n",
       "\n",
       "# compute decision values and probabilities:\n",
       "pred <- predict(model, x, decision.values = TRUE)\n",
       "attr(pred, \"decision.values\")[1:4,]\n",
       "\n",
       "# visualize (classes by color, SV by crosses):\n",
       "plot(cmdscale(dist(iris[,-5])),\n",
       "     col = as.integer(iris[,5]),\n",
       "     pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n",
       "\n",
       "## try regression mode on two dimensions\n",
       "\n",
       "# create data\n",
       "x <- seq(0.1, 5, by = 0.05)\n",
       "y <- log(x) + rnorm(x, sd = 0.2)\n",
       "\n",
       "# estimate model and predict input values\n",
       "m   <- svm(x, y)\n",
       "new <- predict(m, x)\n",
       "\n",
       "# visualize\n",
       "plot(x, y)\n",
       "points(x, log(x), col = 2)\n",
       "points(x, new, col = 4)\n",
       "\n",
       "## density-estimation\n",
       "\n",
       "# create 2-dim. normal with rho=0:\n",
       "X <- data.frame(a = rnorm(1000), b = rnorm(1000))\n",
       "attach(X)\n",
       "\n",
       "# traditional way:\n",
       "m <- svm(X, gamma = 0.1)\n",
       "\n",
       "# formula interface:\n",
       "m <- svm(~., data = X, gamma = 0.1)\n",
       "# or:\n",
       "m <- svm(~ a + b, gamma = 0.1)\n",
       "\n",
       "# test:\n",
       "newdata <- data.frame(a = c(0, 4), b = c(0, 4))\n",
       "predict (m, newdata)\n",
       "\n",
       "# visualize:\n",
       "plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))\n",
       "points(newdata, pch = \"+\", col = 2, cex = 5)\n",
       "\n",
       "# weights: (example not particularly sensible)\n",
       "i2 <- iris\n",
       "levels(i2$Species)[3] <- \"versicolor\"\n",
       "summary(i2$Species)\n",
       "wts <- 100 / table(i2$Species)\n",
       "wts\n",
       "m <- svm(Species ~ ., data = i2, class.weights = wts)\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "svm                   package:e1071                    R Documentation\n",
       "\n",
       "_\bS_\bu_\bp_\bp_\bo_\br_\bt _\bV_\be_\bc_\bt_\bo_\br _\bM_\ba_\bc_\bh_\bi_\bn_\be_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     ‘svm’ is used to train a support vector machine. It can be used to\n",
       "     carry out general regression and classification (of nu and\n",
       "     epsilon-type), as well as density-estimation. A formula interface\n",
       "     is provided.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     ## S3 method for class 'formula'\n",
       "     svm(formula, data = NULL, ..., subset, na.action =\n",
       "     na.omit, scale = TRUE)\n",
       "     ## Default S3 method:\n",
       "     svm(x, y = NULL, scale = TRUE, type = NULL, kernel =\n",
       "     \"radial\", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),\n",
       "     coef0 = 0, cost = 1, nu = 0.5,\n",
       "     class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,\n",
       "     shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,\n",
       "     ..., subset, na.action = na.omit)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       " formula: a symbolic description of the model to be fit.\n",
       "\n",
       "    data: an optional data frame containing the variables in the model.\n",
       "          By default the variables are taken from the environment which\n",
       "          ‘svm’ is called from.\n",
       "\n",
       "       x: a data matrix, a vector, or a sparse matrix (object of class\n",
       "          ‘Matrix’ provided by the ‘Matrix’ package, or of class\n",
       "          ‘matrix.csr’ provided by the ‘SparseM’ package, or of class\n",
       "          ‘simple_triplet_matrix’ provided by the ‘slam’ package).\n",
       "\n",
       "       y: a response vector with one label for each row/component of\n",
       "          ‘x’. Can be either a factor (for classification tasks) or a\n",
       "          numeric vector (for regression).\n",
       "\n",
       "   scale: A logical vector indicating the variables to be scaled. If\n",
       "          ‘scale’ is of length 1, the value is recycled as many times\n",
       "          as needed.  Per default, data are scaled internally (both ‘x’\n",
       "          and ‘y’ variables) to zero mean and unit variance. The center\n",
       "          and scale values are returned and used for later predictions.\n",
       "\n",
       "    type: ‘svm’ can be used as a classification machine, as a\n",
       "          regression machine, or for novelty detection.  Depending of\n",
       "          whether ‘y’ is a factor or not, the default setting for\n",
       "          ‘type’ is ‘C-classification’ or ‘eps-regression’,\n",
       "          respectively, but may be overwritten by setting an explicit\n",
       "          value.\n",
       "          Valid options are:\n",
       "\n",
       "            • ‘C-classification’\n",
       "\n",
       "            • ‘nu-classification’\n",
       "\n",
       "            • ‘one-classification’ (for novelty detection)\n",
       "\n",
       "            • ‘eps-regression’\n",
       "\n",
       "            • ‘nu-regression’\n",
       "\n",
       "  kernel: the kernel used in training and predicting. You might\n",
       "          consider changing some of the following parameters, depending\n",
       "          on the kernel type.\n",
       "\n",
       "          linear: u'*v\n",
       "\n",
       "          polynomial: (gamma*u'*v + coef0)^degree\n",
       "\n",
       "          radial basis: exp(-gamma*|u-v|^2)\n",
       "\n",
       "          sigmoid: tanh(gamma*u'*v + coef0)\n",
       "\n",
       "  degree: parameter needed for kernel of type ‘polynomial’ (default: 3)\n",
       "\n",
       "   gamma: parameter needed for all kernels except ‘linear’ (default:\n",
       "          1/(data dimension))\n",
       "\n",
       "   coef0: parameter needed for kernels of type ‘polynomial’ and\n",
       "          ‘sigmoid’ (default: 0)\n",
       "\n",
       "    cost: cost of constraints violation (default: 1)-it is the\n",
       "          ‘C’-constant of the regularization term in the Lagrange\n",
       "          formulation.\n",
       "\n",
       "      nu: parameter needed for ‘nu-classification’, ‘nu-regression’,\n",
       "          and ‘one-classification’\n",
       "\n",
       "class.weights: a named vector of weights for the different classes,\n",
       "          used for asymmetric class sizes. Not all factor levels have\n",
       "          to be supplied (default weight: 1). All components have to be\n",
       "          named.\n",
       "\n",
       "cachesize: cache memory in MB (default 40)\n",
       "\n",
       "tolerance: tolerance of termination criterion (default: 0.001)\n",
       "\n",
       " epsilon: epsilon in the insensitive-loss function (default: 0.1)\n",
       "\n",
       "shrinking: option whether to use the shrinking-heuristics (default:\n",
       "          ‘TRUE’)\n",
       "\n",
       "   cross: if a integer value k>0 is specified, a k-fold cross\n",
       "          validation on the training data is performed to assess the\n",
       "          quality of the model: the accuracy rate for classification\n",
       "          and the Mean Squared Error for regression\n",
       "\n",
       "  fitted: logical indicating whether the fitted values should be\n",
       "          computed and included in the model or not (default: ‘TRUE’)\n",
       "\n",
       "probability: logical indicating whether the model should allow for\n",
       "          probability predictions.\n",
       "\n",
       "     ...: additional parameters for the low level fitting function\n",
       "          ‘svm.default’\n",
       "\n",
       "  subset: An index vector specifying the cases to be used in the\n",
       "          training sample.  (NOTE: If given, this argument must be\n",
       "          named.)\n",
       "\n",
       "na.action: A function to specify the action to be taken if ‘NA’s are\n",
       "          found. The default action is ‘na.omit’, which leads to\n",
       "          rejection of cases with missing values on any required\n",
       "          variable. An alternative is ‘na.fail’, which causes an error\n",
       "          if ‘NA’ cases are found. (NOTE: If given, this argument must\n",
       "          be named.)\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     For multiclass-classification with k levels, k>2, ‘libsvm’ uses\n",
       "     the ‘one-against-one’-approach, in which k(k-1)/2 binary\n",
       "     classifiers are trained; the appropriate class is found by a\n",
       "     voting scheme.\n",
       "\n",
       "     ‘libsvm’ internally uses a sparse data representation, which is\n",
       "     also high-level supported by the package ‘SparseM’.\n",
       "\n",
       "     If the predictor variables include factors, the formula interface\n",
       "     must be used to get a correct model matrix.\n",
       "\n",
       "     ‘plot.svm’ allows a simple graphical visualization of\n",
       "     classification models.\n",
       "\n",
       "     The probability model for classification fits a logistic\n",
       "     distribution using maximum likelihood to the decision values of\n",
       "     all binary classifiers, and computes the a-posteriori class\n",
       "     probabilities for the multi-class problem using quadratic\n",
       "     optimization. The probabilistic regression model assumes\n",
       "     (zero-mean) laplace-distributed errors for the predictions, and\n",
       "     estimates the scale parameter using maximum likelihood.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     An object of class ‘\"svm\"’ containing the fitted model, including:\n",
       "\n",
       "      SV: The resulting support vectors (possibly scaled).\n",
       "\n",
       "   index: The index of the resulting support vectors in the data\n",
       "          matrix. Note that this index refers to the preprocessed data\n",
       "          (after the possible effect of ‘na.omit’ and ‘subset’)\n",
       "\n",
       "   coefs: The corresponding coefficients times the training labels.\n",
       "\n",
       "     rho: The negative intercept.\n",
       "\n",
       "   sigma: In case of a probabilistic regression model, the scale\n",
       "          parameter of the hypothesized (zero-mean) laplace\n",
       "          distribution estimated by maximum likelihood.\n",
       "\n",
       "probA, probB: numeric vectors of length k(k-1)/2, k number of classes,\n",
       "          containing the parameters of the logistic distributions\n",
       "          fitted to the decision values of the binary classifiers (1 /\n",
       "          (1 + exp(a x + b))).\n",
       "\n",
       "_\bN_\bo_\bt_\be:\n",
       "\n",
       "     Data are scaled internally, usually yielding better results.\n",
       "\n",
       "     Parameters of SVM-models usually _must_ be tuned to yield sensible\n",
       "     results!\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen\n",
       "     Lin)\n",
       "     <email: David.Meyer@R-project.org>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "        • Chang, Chih-Chung and Lin, Chih-Jen:\n",
       "          _LIBSVM: a library for Support Vector Machines_\n",
       "          <URL: http://www.csie.ntu.edu.tw/~cjlin/libsvm>\n",
       "\n",
       "        • Exact formulations of models, algorithms, etc. can be found\n",
       "          in the document:\n",
       "          Chang, Chih-Chung and Lin, Chih-Jen:\n",
       "          _LIBSVM: a library for Support Vector Machines_\n",
       "          <URL: http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz>\n",
       "\n",
       "        • More implementation details and speed benchmarks can be found\n",
       "          on: Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:\n",
       "          _Working Set Selection Using the Second Order Information for\n",
       "          Training SVM_\n",
       "          <URL:\n",
       "          http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf>\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘predict.svm’ ‘plot.svm’ ‘tune.svm’ ‘matrix.csr’ (in package\n",
       "     ‘SparseM’)\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     data(iris)\n",
       "     attach(iris)\n",
       "     \n",
       "     ## classification mode\n",
       "     # default with factor response:\n",
       "     model <- svm(Species ~ ., data = iris)\n",
       "     \n",
       "     # alternatively the traditional interface:\n",
       "     x <- subset(iris, select = -Species)\n",
       "     y <- Species\n",
       "     model <- svm(x, y) \n",
       "     \n",
       "     print(model)\n",
       "     summary(model)\n",
       "     \n",
       "     # test with train data\n",
       "     pred <- predict(model, x)\n",
       "     # (same as:)\n",
       "     pred <- fitted(model)\n",
       "     \n",
       "     # Check accuracy:\n",
       "     table(pred, y)\n",
       "     \n",
       "     # compute decision values and probabilities:\n",
       "     pred <- predict(model, x, decision.values = TRUE)\n",
       "     attr(pred, \"decision.values\")[1:4,]\n",
       "     \n",
       "     # visualize (classes by color, SV by crosses):\n",
       "     plot(cmdscale(dist(iris[,-5])),\n",
       "          col = as.integer(iris[,5]),\n",
       "          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n",
       "     \n",
       "     ## try regression mode on two dimensions\n",
       "     \n",
       "     # create data\n",
       "     x <- seq(0.1, 5, by = 0.05)\n",
       "     y <- log(x) + rnorm(x, sd = 0.2)\n",
       "     \n",
       "     # estimate model and predict input values\n",
       "     m   <- svm(x, y)\n",
       "     new <- predict(m, x)\n",
       "     \n",
       "     # visualize\n",
       "     plot(x, y)\n",
       "     points(x, log(x), col = 2)\n",
       "     points(x, new, col = 4)\n",
       "     \n",
       "     ## density-estimation\n",
       "     \n",
       "     # create 2-dim. normal with rho=0:\n",
       "     X <- data.frame(a = rnorm(1000), b = rnorm(1000))\n",
       "     attach(X)\n",
       "     \n",
       "     # traditional way:\n",
       "     m <- svm(X, gamma = 0.1)\n",
       "     \n",
       "     # formula interface:\n",
       "     m <- svm(~., data = X, gamma = 0.1)\n",
       "     # or:\n",
       "     m <- svm(~ a + b, gamma = 0.1)\n",
       "     \n",
       "     # test:\n",
       "     newdata <- data.frame(a = c(0, 4), b = c(0, 4))\n",
       "     predict (m, newdata)\n",
       "     \n",
       "     # visualize:\n",
       "     plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))\n",
       "     points(newdata, pch = \"+\", col = 2, cex = 5)\n",
       "     \n",
       "     # weights: (example not particularly sensible)\n",
       "     i2 <- iris\n",
       "     levels(i2$Species)[3] <- \"versicolor\"\n",
       "     summary(i2$Species)\n",
       "     wts <- 100 / table(i2$Species)\n",
       "     wts\n",
       "     m <- svm(Species ~ ., data = i2, class.weights = wts)\n",
       "     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model <- svm(Species ~ ., data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Species ~ ., data = iris)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  radial \n",
       "       cost:  1 \n",
       "      gamma:  0.25 \n",
       "\n",
       "Number of Support Vectors:  51\n",
       "\n",
       " ( 8 22 21 )\n",
       "\n",
       "\n",
       "Number of Classes:  3 \n",
       "\n",
       "Levels: \n",
       " setosa versicolor virginica\n",
       "\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Vorhersage Beispiel\n",
    "\n",
    "Da wir einen kleinen Datensatz haben werden wir ihn nicht in Test- und Trainingsdatensatz aufteilen (was immer versucht werden sollte), sondern werden wir hier das Modell mit demselben Datensatz testen, mit dem wir es trainiert haben. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted.values <- predict(model,iris[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                \n",
       "predicted.values setosa versicolor virginica\n",
       "      setosa         50          0         0\n",
       "      versicolor      0         48         2\n",
       "      virginica       0          2        48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table(predicted.values,iris[,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir schon mal bei dem Iris Datensatz gesehen haben, lassen sich die Setosa Werte leicht von dem Rauschen zwischen Versicolor und Virginica trennen. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fortgeschrittenes Tuning\n",
    "\n",
    "Wir können versuchen die Parameter zu tunen um unser Modell zu verbessern. Du kannst mit der `help()` Funktion die Dokumentation aufrufen um nachzuvollziehen was jeder Parameter bedeutet. Wir verwenden die `tune` Funktion:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tunen nach Kombinationen von gamma 0.5,1,2\n",
    "# and costs 1/10 , 10 , 100\n",
    "tune.results <- tune(svm,train.x=iris[1:4],train.y=iris[,5],kernel='radial',\n",
    "                  ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Parameter tuning of ‘svm’:\n",
       "\n",
       "- sampling method: 10-fold cross validation \n",
       "\n",
       "- best parameters:\n",
       " cost gamma\n",
       "    1   0.5\n",
       "\n",
       "- best performance: 0.04666667 \n",
       "\n",
       "- Detailed performance results:\n",
       "    cost gamma      error dispersion\n",
       "1    0.1   0.5 0.05333333 0.06885304\n",
       "2    1.0   0.5 0.04666667 0.04499657\n",
       "3   10.0   0.5 0.05333333 0.05258738\n",
       "4  100.0   0.5 0.06000000 0.04919099\n",
       "5    0.1   1.0 0.06666667 0.05443311\n",
       "6    1.0   1.0 0.06000000 0.06629526\n",
       "7   10.0   1.0 0.06000000 0.06629526\n",
       "8  100.0   1.0 0.07333333 0.06629526\n",
       "9    0.1   2.0 0.10000000 0.06478835\n",
       "10   1.0   2.0 0.06000000 0.06629526\n",
       "11  10.0   2.0 0.06000000 0.06629526\n",
       "12 100.0   2.0 0.06666667 0.07027284\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(tune.results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können nun sehen, dass die beste Performance bei cost=1 und gamma=0.5 stattfindet. Du kannst nun das Modell mit diesen Parametern trainieren um eventuell besseres Modell zu bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned.svm <- svm(Species ~ ., data=iris, kernel=\"radial\", cost=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Species ~ ., data = iris, kernel = \"radial\", cost = 1, \n",
       "    gamma = 0.5)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  radial \n",
       "       cost:  1 \n",
       "      gamma:  0.5 \n",
       "\n",
       "Number of Support Vectors:  59\n",
       "\n",
       " ( 11 23 25 )\n",
       "\n",
       "\n",
       "Number of Classes:  3 \n",
       "\n",
       "Levels: \n",
       " setosa versicolor virginica\n",
       "\n",
       "\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(tuned.svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned.predicted.values <- predict(tuned.svm,iris[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                      \n",
       "tuned.predicted.values setosa versicolor virginica\n",
       "            setosa         50          0         0\n",
       "            versicolor      0         48         2\n",
       "            virginica       0          2        48"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table(tuned.predicted.values,iris[,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sieht danach aus, dass wir unser Modell nicht verbessern konnten. Das Verfahren ein Modell einfach mit vielen verschiedenen Parametern zu Tunen wird Grid Search genannt. In diesem Fall hier haben wir vermutlich zu wenig Daten um unser Modell durch sorgfältige Parameterauswahl zu verbessern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gut gemacht! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
